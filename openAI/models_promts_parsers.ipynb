{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd6eb74",
   "metadata": {},
   "source": [
    "Prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af9f8768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helloo\n"
     ]
    }
   ],
   "source": [
    "print(\"helloo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d95fb3b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0edfc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4560e3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(dotenv_path=r\"C:\\Users\\hasan\\Rafi_SAA\\python_practices\\LLM_Langchain\\llm_openai\\.env\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da118a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url=\"https://models.github.ai/inference\",\n",
    "    api_key=os.environ[\"GPT_github_access_token\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b57a4bb",
   "metadata": {},
   "source": [
    "Helper function for promt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc51bbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"openai/gpt-4o\"):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.3,   # I need to adjust this based on my needs\n",
    "        max_tokens=4096,\n",
    "        top_p=1\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5d2d0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm just a program, so I don't have feelings, but thank you for asking! How can I assist you today? ðŸ˜Š\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18052c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I donâ€™t have memory of past interactions or the ability to recognize individual users. Each conversation starts fresh, so I donâ€™t remember you or anything we've discussed before. However, I'm here to help with any questions or topics you'd like to talk about!\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"do you remember me?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f337f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = \"hey mate how ya doin today? ya wassaup, been hangin around the table?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7efedf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "style= \"Convert the following text into a formal tone.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ea00d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert the following text into a formal tone.\n",
      "\n",
      "Text: hey mate how ya doin today? ya wassaup, been hangin around the table?\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"{style}\\n\\nText: {sms}\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f22b2b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, how are you doing today? Whatâ€™s going on? Have you been spending time around the table?'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c858b",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "\n",
    "Now let's see how we cna generate the responses using langchain\n",
    "\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d972073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x18bd8140cd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c6b1989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49246cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c138158b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv(dotenv_path=r\"C:\\Users\\hasan\\Rafi_SAA\\python_practices\\LLM_Langchain\\llm_openai\\.env\")\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model=\"openai/gpt-4o\",\n",
    "    openai_api_key=os.getenv(\"GPT_github_access_token\"),\n",
    "    openai_api_base=\"https://models.github.ai/inference\"\n",
    "\n",
    ")\n",
    "response = chat.invoke(\"What is the capital of France?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5bb53c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000018BD94A7190>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018BD8601C50>, root_client=<openai.OpenAI object at 0x0000018BD8E16250>, root_async_client=<openai.AsyncOpenAI object at 0x0000018BD94EFD90>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57fccab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c159d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15c7ca1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e4e5fb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "18eb9c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9d531103",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "401ce3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d0323b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563d02e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000018BD94A7190>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000018BD8601C50>, root_client=<openai.OpenAI object at 0x0000018BD8E16250>, root_async_client=<openai.AsyncOpenAI object at 0x0000018BD94EFD90>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62010305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No, I donâ€™t know you personally. I donâ€™t have access to personal data about individuals unless it has been shared with me in the course of our conversation. How can I assist you today?'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "# customer_response = get_completion(customer_messages)\n",
    "get_completion(\"Do you know me?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bbfb6916",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "070ee868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am incredibly frustrated that the lid of my blender came off and splattered smoothie all over my kitchen walls. To make things even worse, the warranty doesnâ€™t cover the cost of cleaning up my kitchen. I would greatly appreciate your assistance in resolving this matter.\n"
     ]
    }
   ],
   "source": [
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e8d5b",
   "metadata": {},
   "source": [
    "Testing with different data agaiun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4f95074",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "851a644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8cb267c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "811b3ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, me esteemed customer,  \n",
      "\n",
      "Ye warranty, alas, doth not be coverin' th' cost o' cleanin' yer galley, as 'tis on yer own sails fer misusin' yer blender. It seems ye forgot t' place th' lid atop afore spinnin' yon blades. A most unfortunate mishap, indeed!  \n",
      "\n",
      "Fair winds to ye, and may yer next venture be smoother sailin'!  \n",
      "\n",
      "Arr!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee3b470",
   "metadata": {},
   "source": [
    "---------------------\n",
    "Output Parsers\n",
    "===============\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852d247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
